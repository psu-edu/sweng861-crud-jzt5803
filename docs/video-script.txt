================================================================================
  CAMPUS ANALYTICS PLATFORM
  Final Project Presentation – Video Narration Script
  SWENG 861: Software Construction | Spring 2026 | Penn State University
  Student: Jomar Thomas Almonte
  Target runtime: 12–13 minutes at normal speaking pace
================================================================================

NOTES TO SELF BEFORE RECORDING:
  - Have the PowerPoint open and ready on slides 1 of 9
  - Have a terminal open, logged into the project directory
  - Have a browser ready with tabs pre-loaded for localhost:3000, :9090, :4000
  - Read slowly and deliberately — do NOT rush
  - Pause briefly at each [CLICK] or [TYPE] direction to let the action register
    on screen before continuing
  - Timing markers are approximate; adjust as needed during recording
================================================================================


================================================================================
  SECTION 1: INTRODUCTION
  Target: ~1 minute  [00:00 – 01:00]
================================================================================

[00:00]

Hello everyone. My name is Jomar Thomas Almonte, and this is my final project
presentation for SWENG 861, Software Construction, here at Penn State, Spring
2026.

[00:10]

The project I built is called the Campus Analytics Platform. It is a full-stack
web application that lets university faculty and department heads track
institutional metrics — things like enrollment figures, financial data, and
academic performance — all in one place, with a secure login, a live weather
widget pulling conditions right here from the Penn State campus, and a fully
automated DevOps pipeline behind the scenes.

[00:35]

What I want to walk you through today is how this application came together
across seven weeks of the course — each week adding a new layer, from the
initial API design all the way through containerization, observability, and
production-grade security practices.

[00:50]

I will start with the slides to give you the high-level picture, and then I
will switch over to a live demo of the running application so you can see
everything in action.

Let's get started.

[CLICK to next slide]


================================================================================
  SECTION 2: SLIDES WALKTHROUGH
  Target: ~7 minutes  [01:00 – 08:00]
================================================================================

---SLIDE 2 — Requirements and Success Criteria---

[01:00]

The problem this application is solving is a real one in higher education.
University departments typically track their metrics in spreadsheets — shared
Excel files, Google Sheets, sometimes just email threads. There is no single
source of truth, no audit trail, and no way to quickly answer questions like
"how did fall enrollment compare to last year?" or "what is our facilities
budget utilization right now?"

[01:20]

The target users for Campus Analytics are faculty members, department heads,
and university administrators — people who need a simple, secure interface to
enter and review data, without needing to understand SQL or data warehouse
tooling.

[01:35]

The seven core features I implemented are:

One — secure user authentication with both username/password and Google OAuth,
including a JWT bridge that ensures OAuth users can make API calls seamlessly.

Two — full CRUD operations on campus metrics, organized into five categories:
enrollment, facilities, academic, financial, and a general "other" bucket.
Values are displayed in proper US format — commas for thousands separators and
dollar signs for monetary amounts.

Three — a live weather integration using Open-Meteo, showing current conditions
at Penn State's coordinates in US measurements — temperature in Fahrenheit and
wind speed in miles per hour.

Four — a domain event system that writes an audit trail every time a metric
is created or modified.

Five — an admin master account that has visibility into all metrics across all
users in the system — a full administrative view.

Six — Prometheus-compatible metrics scraping and a Grafana observability
dashboard.

And seven — a complete CI/CD pipeline that builds, tests, and packages the
application automatically on every code push.

[02:05]

On the non-functional side, the three pillars I focused on were security —
no hardcoded credentials, rate limiting on every route, JWT-based auth —
observability — structured logging, health endpoints, and real-time metrics —
and testing, with over 320 tests across unit, frontend component, and
integration layers.

[CLICK to next slide]


---SLIDE 3 — Architecture---

[02:20]

Let me walk you through the architecture from top to bottom.

At the top we have the presentation layer — a Next.js 15 application using the
App Router. This handles both the server-side rendered pages that users see in
the browser and all of the API routes that API clients or the CI pipeline's
smoke test can call directly.

[02:40]

I chose Next.js specifically because it collapses two deployment concerns into
one framework. Normally you would deploy a separate React frontend and a
separate Node.js backend API. With Next.js, both live in the same codebase and
deploy as a single Docker image. That simplifies the CI pipeline, reduces
infrastructure cost, and eliminates the cross-origin complexity that comes with
a split frontend-backend setup.

[03:00]

In the middle layer we have the business logic — all of it living in the lib/
directory. That includes the authentication library, the structured logger, the
Prometheus metrics module, validation, rate limiting, and the weather service.

At the bottom we have the data layer — SQLite managed through Sequelize as the
ORM. I chose SQLite for this academic context because there are zero
infrastructure dependencies: the database is a single file, it starts instantly,
and it is perfectly adequate for the concurrency levels of a university
department tool. I did document the migration path to PostgreSQL in the
scalability section of my report.

[03:30]

The four data models are User, Metric, WeatherData, and DomainEvent. Users own
metrics, weather records, and domain events — all with cascade deletes, so
removing a user cleans up all their associated data automatically.

[03:45]

The domain event model deserves a quick mention. Every time something
significant happens in the application — a metric is created, an auth event
fires — the system writes a DomainEvent record with an event type, the entity
it affected, and the full payload. This gives us a complete, append-only audit
trail without having to modify any of the core tables. It is the same pattern
used in event-sourced systems at scale.

[04:05]

For external integrations we have Open-Meteo — a free, no-API-key weather
service — and optional Google OAuth through NextAuth for social login.

[CLICK to next slide]


---SLIDE 4 — Technology Stack---

[04:15]

Looking at the technology stack:

The frontend is Next.js 15 with React 19 and TailwindCSS 4. The backend is
Next.js API routes running on Node.js 18. The database is SQLite through
Sequelize 6. Authentication is a combination of JSON Web Tokens and NextAuth.

[04:30]

That authentication combination is worth one minute of explanation because it
is a deliberate design decision, not an accident.

JWT handles API clients. When you call the API from a terminal, from Postman,
or from the CI smoke test, you authenticate with a username and password, the
server signs a JWT and hands it back, and from then on you pass that token in
an Authorization Bearer header. Stateless, fast, and works with any HTTP client.

NextAuth handles browser sessions. When a user logs in through the web
interface, NextAuth manages the encrypted session cookie. This lets us support
optional Google OAuth social login on top of the email-and-password flow
without writing that OAuth flow from scratch.

The two systems coexist because the getAuthUser function in lib/auth.js checks
the Authorization header first, and falls through to the NextAuth session if no
Bearer token is present. One code path, two authentication mechanisms.

[05:10]

For testing, I used Jest 30 with React Testing Library for unit and frontend
component tests, and Node's built-in test runner for integration tests.

[CLICK to next slide]


---SLIDE 5 — Core Features---

[05:20]

Let me go a bit deeper on a few of the core features.

Authentication uses multiple layers. The API layer uses JWT tokens with a
one-hour expiration. The browser layer uses NextAuth sessions. Rate limiting
is applied at the route level — the auth routes use a stricter limiter to
slow down brute-force attempts, and the general API uses a more permissive
limit for normal usage.

[05:38]

For the metrics CRUD — every metric record has a name, a numeric value, a unit
of measurement, an optional description, and a JSON metadata field for
arbitrary extra context. Category is an enum constrained to those five values I
mentioned: enrollment, facilities, academic, financial, and other. The list
view supports pagination and category filtering, so a department head can
quickly filter to see only their financial metrics, for example.

[05:58]

The weather integration uses Open-Meteo, which requires no API key, so there
are no secrets to manage for that feature. The dashboard widget is hardcoded to
Penn State's latitude and longitude — 40.79 North, 77.86 West — which is
actually a meaningful design choice: this is a Penn State application, so
always showing Penn State weather makes sense. Results are cached in memory
for ten minutes to avoid hammering the external API on every page load.

[06:20]

The domain event system runs asynchronously. When you create a metric, the API
route fires an event — metric.created — and the event emitter writes a
DomainEvent row to the database in the background. The API response does not
wait for the event to be written; it returns the created metric immediately.
This keeps response latency low while still building the audit trail.

[CLICK to next slide]


---SLIDE 6 — DevOps and Observability---

[06:35]

This is the section I spent the most time on in week six, and I think it is
the most directly applicable to professional software engineering, so I want
to give it the detail it deserves.

The CI/CD pipeline is defined in .github/workflows/ci.yml and triggers on
every push and every pull request targeting the main branch.

Job one is build-and-test. It checks out the code, sets up Node 18 with
dependency caching for faster runs, installs with npm ci for a clean
lockfile-reproducible install, builds the Next.js application, runs the full
Jest test suite with coverage collection, uploads the coverage report as a
workflow artifact retained for 14 days, and then runs npm audit to check for
known high or critical CVEs.

[07:10]

The critical thing about the test step is that it has no continue-on-error
flag. That means a single test failure kills the entire pipeline. Broken code
cannot progress to the Docker build stage. That is the quality gate, and it is
non-negotiable.

Job two is docker-build, and it only starts if job one passes. It builds the
production Docker image, tags it with both the Git commit SHA — for traceability
— and a human-friendly label. Then it runs a smoke test: start the container,
wait 15 seconds, hit the health endpoint with curl, confirm a 200 response, and
tear it down. If the app cannot survive that smoke test, the pipeline fails.

[07:40]

The Dockerfile is a three-stage multi-stage build. Stage one installs
dependencies. Stage two builds the Next.js standalone output. Stage three copies
only the compiled artifacts into a minimal Alpine image — so we are not shipping
source code, node_modules, test files, or anything else that is not needed at
runtime.

The most important security property of the Docker image is that the process
runs as a non-root user — a dedicated system user named nextjs with UID 1001.
If an attacker were to exploit a vulnerability in the application, they get
a sandboxed non-root process. They cannot gain root access to the host through
that container. That is a meaningful reduction in blast radius.

[08:00]

On observability — I implemented three health endpoints with different purposes.
The main /api/health endpoint returns the application status, database status,
uptime in seconds, timestamp, and version number. It always returns HTTP 200
even if the database is down, because a load balancer uses this endpoint to
decide whether to route traffic, and you do not want a flapping database to
cause the load balancer to remove the app from rotation when the app itself is
still serving requests fine.

The /api/health/live endpoint is even simpler — it always returns 200 as long
as the Node.js process is alive. Kubernetes uses this to decide whether to
restart a container.

The /api/health/ready endpoint returns 503 if the database is unreachable.
Kubernetes uses this to decide whether to send traffic to a container. If the
database is down, the app should not receive requests it cannot serve. So ready
returns 503, but live still returns 200 — that tells the orchestrator "the
process is fine, do not restart it, but do not route traffic to it until the
database comes back."

[08:30]

Structured logging in lib/logger.js writes every log line as a single line of
JSON with consistent fields: timestamp in ISO-8601 format, level, message, and
any metadata you pass in. Metadata is automatically sanitized — any key named
password, token, secret, or authorization is stripped before the log entry is
written. That prevents accidental credential leakage into log files.

JSON format matters because cloud log aggregators — CloudWatch, Stackdriver,
Datadog — can parse JSON fields directly, letting you write queries like
"show me all error logs for route /api/metrics in the last hour" without
brittle regex parsing.

The Prometheus metrics module in lib/metrics.js exposes four metric families:
http_requests_total, which counts every request by method, route, and HTTP
status code; http_request_duration_ms, a histogram of response times with
buckets at 50, 100, 200, 500, 1000, and 2000 milliseconds; metrics_created_total,
a domain-level counter that tells you how much data is being entered into the
system; and auth_logins_total, broken down by success or failure.

The Grafana dashboard has five panels built on top of those metrics:
HTTP request rate over time, HTTP error rate for 4xx and 5xx responses,
request duration at the 95th percentile, total metrics created as a big-number
stat, and total successful logins. During an incident — say a bad deploy
causes a spike in 5xx errors — panels two and three light up together, and
you can drill into the route-level labels to identify which endpoint is the
source in seconds.

[CLICK to next slide]


---SLIDE 7 — AI/GenAI Audit---

[09:00]

I used AI assistance to draft the initial CI/CD YAML configuration, and the
audit of that draft surfaced three significant issues that required correction
before it was safe to commit.

Issue one: the AI hard-coded secret values directly into the YAML file —
things like JWT_SECRET equals supersecret123. Hard-coded secrets in version
control are visible to anyone with repository access, cannot be rotated without
a code change, and get flagged immediately by GitHub's secret scanning. The
fix was to reference GitHub Actions repository secrets using the
secrets context — with a safe placeholder fallback for CI environments where
the secrets have not been configured yet.

[09:28]

Issue two: the AI added continue-on-error true to the test step. That is the
single worst thing you can do to a CI pipeline — it means tests failing produces
a green pipeline. The whole point of CI is to reject broken code automatically.
The fix was to remove that flag entirely from the test step. The security audit
step intentionally keeps continue-on-error true, because vulnerability state
is managed externally and should not block legitimate development.

[09:50]

Issue three: the AI-generated Dockerfile did not include a USER directive,
meaning the container process would run as root. If any dependency in the
application had a remote code execution vulnerability, an attacker would land
in the container with root privileges. The fix was adding the non-root user
creation and the USER directive before the CMD instruction.

[10:08]

The lesson I take from this audit is that AI tools are drafting tools. They
optimize for producing working code, not for applying security defaults. An
engineer reviewing AI-generated DevOps configurations has to actively apply a
security-first mindset, because the AI will not do that for you by default.
Treat AI output as a first draft, not production-ready output.

[CLICK to next slide]


---SLIDE 8 — Testing---

[10:20]

The testing strategy has three layers.

Unit tests in __tests__/unit/ cover the individual library modules in isolation:
authentication logic, the event emitter, API middleware, rate limiting,
input validation, and the weather service. These run fast, they have no
external dependencies, and they run on every CI build.

Frontend component tests in __tests__/frontend/ use React Testing Library to
test the UI components in a jsdom environment — things like the login form,
the metric creation form, and the dashboard layout — verifying they render
correctly, display validation errors, and respond to user interactions.

Integration tests in __tests__/api.test.js use Node's built-in test runner to
make real HTTP calls against a running server on port 3001. These tests verify
the full request-response cycle end to end. They are not run in CI because they
require a running server, but they are documented and runnable locally.

[11:00]

Across all three layers, the test suite has 320 tests across 15 test files.
Jest runs automatically in GitHub Actions on every push to main. If any test
fails, the entire pipeline stops before a Docker image is built.

[CLICK to next slide]


---SLIDE 9 — Reflections---

[11:12]

Two things I am most proud of in this project.

First, the zero-dependency Prometheus metrics implementation. lib/metrics.js
is about 170 lines of pure JavaScript with no npm dependencies at all. It
implements a global singleton that survives Next.js hot reloads in development,
exposes correct Prometheus text-format output, and supports counter and
histogram types with labeled time series. Writing that from scratch gave me a
much deeper understanding of how Prometheus actually works than using a client
library would have.

Second, the multi-layer authentication. Getting JWT and NextAuth to coexist
cleanly — where API clients and browser sessions use the same underlying auth
check — required careful design of the getAuthUser function. The fact that it
is completely transparent to the route handlers is something I think is
genuinely well-designed.

[11:42]

The biggest lesson from this project: observability cannot be bolted on after
the fact. Structured logging, health endpoints, and metrics instrumentation all
needed to be designed into the application architecture from the beginning.
If I had tried to add them at the end, I would have been touching every single
route handler. By designing lib/logger.js and lib/metrics.js as shared modules
from the start, adding instrumentation to new routes takes two or three lines.

These patterns — CI/CD pipelines, structured logging, health checks, Prometheus
scraping — appear in every production software system I am likely to work on
in my career. This project gave me real, hands-on practice with each of them.

[CLICK: switch to live demo tab / terminal]


================================================================================
  SECTION 3: LIVE DEMO
  Target: ~3.5 minutes  [08:30 – 12:00]
================================================================================

[NOTE: The demo section timestamps below continue from the presentation.
       Adjust if your slides took more or less than estimated.]

[00:00 demo / ~08:30 total]

Alright, let me switch over to the live demo. I have a terminal and a browser
open. Let me first get the full stack running.


--- DEMO STEP 1: Start the Application ---

[OPEN terminal in project directory]

[TYPE: docker-compose up -d]

[00:08:50]

What this command does is start three services simultaneously: the Campus
Analytics Next.js application on port 3000, a Prometheus instance on port 9090
configured to scrape our app's metrics endpoint every 15 seconds, and a Grafana
instance on port 4000 with the Campus Analytics dashboard pre-provisioned.
All of this comes up with a single command. Give it a moment to initialize.


--- DEMO STEP 2: Health Check ---

[00:09:00]

While that is starting, let me show you the health endpoint directly from the
terminal.

[TYPE: curl http://localhost:3000/api/health | python3 -m json.tool]

[Wait for response, then read it aloud:]

You can see the response: status is UP, db is UP, uptime shows how many seconds
the process has been running, timestamp in ISO-8601 format, and version 1.0.0.

This is exactly what the CI pipeline's smoke test checks after building the
Docker image. If any of those fields showed an error state, the smoke test
would fail and the pipeline would stop before marking the build as successful.


--- DEMO STEP 3: Register and Login ---

[00:09:20]

[OPEN browser: http://localhost:3000]

Now let me open the application in the browser. Here is the landing page.
Let me navigate to login.

[CLICK: Login / Sign In page]

Here is the login form. I will log in as the superadmin master account —
username superadmin, password Campus123 exclamation point. This account was
created by the seed script that ships with the project.

[TYPE: superadmin in the username field]
[TYPE: Campus123! in the password field]
[CLICK: Sign In]

After a successful login, the server signs a JWT — which is valid for one hour
— and NextAuth sets the session cookie. The application then redirects to the
dashboard. Notice the "Admin" badge next to the username in the navigation bar,
confirming this is an elevated account.

[SHOW: dashboard after redirect, with "superadmin Admin" visible in navbar]

Here we are at the dashboard. You can see the navigation bar showing superadmin
with the Admin badge, the weather widget in the corner, and the quick links to
metrics and weather data.


--- DEMO STEP 4: View the Pre-Seeded Metrics ---

[00:09:50]

Because I am logged in as the superadmin account, the metrics list will show all
metrics in the system — not just the ones this user created. This is the master
admin view.

[CLICK: Metrics in the navigation]

[SHOW: metrics list with 20 pre-seeded rows]

Here you can see the 20 pre-seeded campus metrics. Look at the value column —
notice how numbers are formatted with thousands separators. "47,892 students"
for total enrollment. And in the financial category, you will see dollar amounts
like "$2,100,000,000 USD" for the annual operating budget. That is proper US
locale formatting, not raw database numbers.

Now let me also create a new metric to show the CRUD flow.

[CLICK: Create Metric button]

[TYPE in Name field: Spring 2026 Research Activity]

[SELECT Category: academic]

[TYPE in Value field: 425]

[TYPE in Unit field: active projects]

[CLICK: Create Metric]

[00:10:15]

The form submits to POST /api/metrics. The route validates the input, creates
the database record, fires the domain event in the background, increments the
Prometheus metrics_created_total counter, writes a structured log line, and
returns the created metric.

[SHOW: success message and redirect to metrics list with new metric at top]

There it is — "Spring 2026 Research Activity — 425 active projects — academic."


--- DEMO STEP 5: Metrics List and Filtering ---

[00:10:20]

[SHOW: metrics list page]

The metrics list supports pagination and category filtering. Let me click on the
enrollment category filter.

[CLICK: enrollment filter]

You can see it filters down to only enrollment metrics. Each category has a
color-coded badge so you can scan the list quickly. This is backed by a query
parameter on GET /api/metrics?category=enrollment, and the API validates that
the category value is one of the five allowed enum values before applying it
to the database query.


--- DEMO STEP 6: Weather Widget ---

[00:10:45]

[CLICK: back to Dashboard]

Let me go back to the dashboard and point out the weather widget.

[SHOW: weather widget]

This widget is pulling live data from Open-Meteo for Penn State's coordinates.
Notice the measurements are in US units — temperature in Fahrenheit, wind speed
in miles per hour. The current temperature, humidity, wind speed, and a
human-readable weather description — things like "partly cloudy" or "light rain"
— are fetched when the page loads. The result is cached in memory for ten minutes,
so repeated dashboard loads do not make redundant API calls to the external service.


--- DEMO STEP 7: Prometheus Metrics ---

[00:11:10]

[OPEN new browser tab: http://localhost:9090]

Now let me switch over to Prometheus. Prometheus has been scraping our
application at /api/prometheus every 15 seconds since the stack came up.

[CLICK: Graph tab or expression input]

[TYPE in expression field: http_requests_total]

[CLICK: Execute]

[SHOW: results table or graph]

You can see the http_requests_total counter broken down by labels: method,
route, and status code. Every API call we made during the demo has been
recorded here. This is the raw telemetry data that Grafana visualizes.

[CLEAR expression field]

[TYPE: metrics_created_total]

[CLICK: Execute]

And here is the domain-level counter — metrics_created_total. That number
went up when I created the enrollment metric a moment ago.


--- DEMO STEP 8: Grafana Dashboard ---

[00:11:35]

[OPEN new browser tab: http://localhost:4000]

Now the Grafana dashboard. The credentials are admin and campus123 — they are
automatically configured by the docker-compose file.

[CLICK: Login with admin / campus123]

[NAVIGATE: Dashboards -> Campus Analytics - Observability]

[SHOW: dashboard with all 5 panels]

[00:11:50]

Here is the full dashboard. Let me walk through each panel.

Panel one — HTTP Request Rate — is a time series showing requests per second
by route. You can see the spike from the demo activity we just did.

Panel two — HTTP Error Rate — shows 4xx and 5xx responses over time. Right now
it is flat and near zero, which is what we want.

Panel three — Request Duration p95 — shows the 95th percentile response time
in milliseconds by route. This is our latency SLO metric. Our target is that
p95 stays below 500 milliseconds.

Panel four — Metrics Created — is a big-number stat showing the total count
of metric records ever created. You can see it reflects the record we just
added.

Panel five — Successful Logins — is another stat showing total successful
authentication events.

[00:12:15]

The way you would use this during an incident: imagine a deploy has gone out
and users start reporting slowness. You open Grafana. Panel three shows p95
latency climbing above 500 ms. Panel two shows a correlated spike in 5xx
responses. You look at the route-level breakdown in panel one to identify
that the spike is coming from POST /api/metrics specifically. You roll back
the deploy. You confirm on the dashboard that latency and error rates return
to baseline. Incident resolved.

That is what production observability looks like, and this project gave me
a working implementation of it.


================================================================================
  SECTION 4: WRAP-UP
  Target: ~1 minute  [12:15 – 13:00]
================================================================================

[00:12:20]

[RETURN to slides or face camera]

Let me close out with a few reflections.

The hardest single problem in this project — and one I found during Week 7 QA —
was a data persistence bug. The `ensureDb()` function was calling Sequelize's
`sync({ alter: true })` which, on SQLite, silently drops and recreates tables
with ENUM columns on every server restart, wiping all seeded data. The fix was
a one-word change: `sync()` without alter. That is the kind of subtle
infrastructure bug that only surfaces during end-to-end testing.

The second hardest problem was a secret alignment bug: the middleware was using
`process.env.NEXTAUTH_SECRET` with no fallback, while the NextAuth handler had
a three-level fallback chain. With no `.env.local` file present, the middleware
always got `undefined` as its secret, causing `getToken()` to return null on
every request — which meant the app redirected every authenticated user back to
the login page. The fix was aligning the fallback chains.

[00:12:48]

What I am most proud of: the zero-dependency Prometheus implementation, the
comprehensive 320-test suite, and the fact that when real QA bugs surfaced in
Week 7, I had the tooling and codebase structure to find and fix them quickly.

[00:12:58]

How this connects to my future work: every production system I am going to
encounter in my career is going to have CI/CD pipelines, structured logging,
health checks, and metrics dashboards. The specific tools will vary — GitHub
Actions versus Jenkins, Prometheus versus Datadog, Next.js versus something
else entirely — but the patterns are universal. This project gave me a real
implementation of all of them, not just conceptual familiarity.

[00:13:20]

Thank you for watching. I am happy to answer any questions.

================================================================================
  END OF SCRIPT
================================================================================

TOTAL ESTIMATED RUNTIME: ~13 minutes at normal speaking pace.
To trim to 12 minutes: reduce the DevOps slide section by 30 seconds
by abbreviating the Grafana panel walkthrough (panels 4 and 5 can be
mentioned briefly rather than described individually).
================================================================================
